

Search files...
ç°åœ¨æˆ‘éœ€è¦æŸ¥æ‰¾ä¸è°ƒç”¨ TaylorSeerGuider ç›¸å…³çš„ä»£ç ï¼Œç‰¹åˆ«æ˜¯ denoise æ–¹æ³•ï¼š

Search files...
ç°åœ¨æˆ‘éœ€è¦çœ‹ä¸€ä¸‹ DiffusionEngine ä¸­è°ƒç”¨ sampler çš„ç›¸å…³ä»£ç ï¼š

Search files...
è®©æˆ‘å†æŸ¥çœ‹ä¸€ä¸‹å’Œ prepare_inputs å’Œè°ƒç”¨ç›¸å…³çš„ä»£ç ï¼š

Search files...
æœ€åï¼Œè®©æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸‹ model.py ä¸­ Line 1123 çš„ä¸Šä¸‹æ–‡ï¼š

Read file: dit/model.py
åŸºäºæ”¶é›†åˆ°çš„ä¿¡æ¯ï¼Œæˆ‘å°†æ•´ç†å‡ºä¸€ä»½å…³äº TaylorSeerGuider åŠå…¶ç›¸å…³è°ƒç”¨ä»£ç çš„è¯¦ç»†æ–‡æ¡£ï¼š

# TaylorSeerGuider åŠç›¸å…³è°ƒç”¨ä»£ç æ–‡æ¡£

## 1. TaylorSeerGuider æ ¸å¿ƒç±»å®ç°

```python
# æ–‡ä»¶ä½ç½®: dit/sampling/guiders.py
class TaylorSeerGuider:
    def __init__(self, scale=5.0, use_cache=True, step_threshold=0.05):
        self.scale = scale
        self.use_cache = use_cache
        self.step_threshold = step_threshold
        self.cache = {}        # step_key -> Îµ
        self.history = []      # [(sigma, Îµ)]
        self._skip_cur_step = False  # ä¸´æ—¶æ ‡è®°å½“å‰æ˜¯å¦è·³è¿‡æ¨¡å‹æ¨ç†
        self._identity_mode = (scale == 0.0)  # æ˜¯å¦ç­‰ä»·äº IdentityGuider

        if self._identity_mode:
            print("[TaylorSeerGuider] âš ï¸  scale=0 â†’ Running in IdentityGuider mode (single batch, no guidance)")

        # åŠ å…¥ç»Ÿè®¡é¡¹
        self.total_steps = 0
        self.skipped_steps = 0

    def has_cache_for(self, sigma):
        step_key = sigma[0].item() if isinstance(sigma, torch.Tensor) else sigma
        return step_key in self.cache

    def prepare_inputs(self, x, sigma, cond, uc, rope_position_ids):
        step_key = sigma[0].item() if isinstance(sigma, torch.Tensor) else sigma

        self.total_steps += 1  # æ¯æ¬¡ prepare_inputs éƒ½ç®—ä¸€æ¬¡ step

        if self.use_cache and self.has_cache_for(sigma):
            self._skip_cur_step = True
            return None, None, None, None
        else:
            self._skip_cur_step = False

        if self._identity_mode:
            # ä¸ IdentityGuider ä¸€è‡´ï¼šåªä½¿ç”¨ condï¼Œä¸æ‹¼æ¥
            c_out = {k: cond[k] for k in cond}
            return x, sigma, c_out, rope_position_ids

        # æ ‡å‡†æ‹¼æ¥æµç¨‹ï¼šcond + uncond
        c_out = {k: torch.cat((uc[k], cond[k]), dim=0) for k in cond}
        x_cat = torch.cat([x] * 2, dim=0)
        sigma_cat = torch.cat([sigma] * 2, dim=0)
        if rope_position_ids is not None:
            rope_position_ids = torch.cat([rope_position_ids] * 2, dim=0)

        return x_cat, sigma_cat, c_out, rope_position_ids

    def __call__(self, x, sigma):
        step_key = sigma[0].item() if isinstance(sigma, torch.Tensor) else sigma

        # ä¼˜å…ˆä½¿ç”¨ cache
        if self.use_cache and step_key in self.cache:
            print(f"[TaylorSeer] âš¡ Using cached Îµ at sigma={step_key:.4f}")
            return self.cache[step_key]

        if x is None:
            raise ValueError("x is None but no cache hit! This shouldn't happen.")

        if self._identity_mode:
            print(f"[TaylorSeer] âœ… IdentityGuider mode at sigma={step_key:.4f}")
            return x  # ä¸ IdentityGuider ä¸€è‡´

        # æ­£å¸¸å¼•å¯¼ï¼šCFG åˆæˆ
        x_u, x_c = x.chunk(2)
        guided = x_u + self.scale * (x_c - x_u)
        print(f"[TaylorSeer] âœ… Real model output at sigma={step_key:.4f}")

        if self.use_cache:
            self.history.append((step_key, guided.detach()))
            self.cache[step_key] = guided.detach()

            if len(self.history) >= 2:
                s0, e0 = self.history[-2]
                s1, e1 = self.history[-1]
                delta = abs(s1 - s0)
                if delta < self.step_threshold:
                    s2 = s1 - delta
                    extrapolated = e1 + (e1 - e0)
                    self.cache[s2] = extrapolated.detach()
                    print(f"[TaylorSeer] â© Precomputed extrapolation for sigma={s2:.4f}")

        return guided

    def get_stats(self):
        skip_ratio = self.skipped_steps / max(self.total_steps, 1)
        return {
            "total_steps": self.total_steps,
            "skipped_steps": self.skipped_steps,
            "skip_ratio": skip_ratio,
        }
```

## 2. denoise æ–¹æ³•å®ç°ï¼ˆBaseDiffusionSamplerï¼‰

```python
# æ–‡ä»¶ä½ç½®: dit/sampling/samplers.py
def denoise(self, x, denoiser, sigma, cond, uc, rope_position_ids, sample_step=None):
    # å‡†å¤‡è¾“å…¥ï¼ˆå¯èƒ½æå‰åˆ¤æ–­ skipï¼‰
    images, sigmas, cond, rope_position_ids = self.guider.prepare_inputs(x, sigma, cond, uc, rope_position_ids)

    # å¦‚æœä¸º IdentityGuider åˆ™æ²¡æœ‰å¯ç”¨æˆåŠŸ taylorguider
    if self.guider.__class__.__name__ == "IdentityGuider":
        print("æ²¡æœ‰å¯ç”¨æˆåŠŸtaylorguider")

    if getattr(self.guider, "_skip_cur_step", False): 
        # âš¡ è·³è¿‡æ¨¡å‹æ¨ç†
        print(self.guider._skip_cur_step)
        denoised = self.guider(None, sigma)
    else:
        # ğŸ§  æ­£å¸¸è°ƒç”¨ denoiserï¼ˆDiT æ¨¡å‹ï¼‰
        denoised = denoiser(images, sigmas, rope_position_ids, cond, sample_step) # ä¼šè°ƒç”¨ dit/model.py:1118
        denoised = self.guider(denoised, sigma)

    return denoised
```

## 3. wrapped_denoiser å‡½æ•° (DiffusionEngine.sample æ–¹æ³•å†…)

```python
# æ–‡ä»¶ä½ç½®: dit/model.py
def wrapped_denoiser(images, sigmas, rope_position_ids, cond, sample_step):
    if hasattr(self, 'guider') and self.guider is not None:
        print(f"[DiffusionEngine.sample] Using guider: {self.guider}")
        # guider é¢„å¤„ç†è¾“å…¥
        x, sigma, c, rope_ids = self.guider.prepare_inputs(images, sigmas, cond, None, rope_position_ids)
        if x is None:
            return None

        # æ„é€  kwargsï¼šå…¨éƒ¨é€šè¿‡å­—å…¸ä¼ å‚ï¼Œé¿å…é‡å¤
        c = c.copy()
        c['images'] = x
        c['sigmas'] = sigma

        # è°ƒç”¨å»å™ªæ¥å£ï¼ˆä¸å†æ˜¾å¼ä¼ é€’ lr_imgs ç­‰ï¼‰
        denoised = self.precond_forward(
            inference=0,
            rope_position_ids=rope_ids,
            ar=ar,
            ar2=ar2,
            sample_step=sample_step,
            block_batch=block_batch,
            **c
        )
        return self.guider(denoised, sigma)

    else:
        print(f"[DiffusionEngine.sample] No guider available, using default denoising")
        # é»˜è®¤è·¯å¾„ï¼ŒåŒæ ·åªé€šè¿‡ kwargs ä¼ å‚
        cond = cond.copy()
        cond['images'] = images
        cond['sigmas'] = sigmas

        return self.precond_forward(
            inference=0,
            rope_position_ids=rope_position_ids,
            ar=ar,
            ar2=ar2,
            sample_step=sample_step,
            block_batch=block_batch,
            **cond
        )
```

## 4. é‡‡æ ·ä¸»å¾ªç¯ (DiffusionEngine.sample æ–¹æ³•)

```python
# æ–‡ä»¶ä½ç½®: dit/model.py
def sample(self, shape, rope_position_ids=None, num_steps=None, images=None, lr_imgs=None,
           init_noise=True, dtype=torch.float32, device=torch.device('cuda'),
           return_attention_map=False, image_2=None, do_concat=True, ar=False,
           ar2=False, block_batch=1):
    # âœ… æ˜¾ç¤º guider ä¿¡æ¯
    print(f"[DiffusionEngine.sample] Starting sampling with guider: {getattr(self, 'guider', None)}")
    
    # ... [åˆå§‹åŒ–è¾“å…¥éƒ¨åˆ†çœç•¥]
    
    # å¯ç”¨Transformerè¾“å‡ºéšè—çŠ¶æ€
    self.transformer.output_hidden_states = True
    
    # å®šä¹‰ wrapped_denoiser å‡½æ•°
    # ... [å‰æ–‡å·²åˆ—å‡º]
    
    # æ‰§è¡Œé‡‡æ ·è¿‡ç¨‹
    samples = self.sampler(
        denoiser=wrapped_denoiser,
        x=None,
        cond=cond,
        uc=uncond,
        num_steps=num_steps,
        rope_position_ids=rope_position_ids,
        init_noise=init_noise
    )
    
    # ... [åå¤„ç†éƒ¨åˆ†çœç•¥]
    
    return samples
```

## 5. TaylorSeerGuider åˆå§‹åŒ–ä¸é…ç½®

```python
# æ–‡ä»¶ä½ç½®: dit/model.py ä¸­çš„ _build_modeling æ–¹æ³•
def _build_modeling(self, args, modeling_configs):
    # ... [å…¶ä»–ä»£ç çœç•¥]
    
    sampler_config = modeling_configs.pop('sampler_config')
    
    # Debug logging for sampler config
    print(f"[DiffusionEngine] Initial sampler config: {sampler_config}")
    
    # ç¡®ä¿ sampler_config æœ‰ params å­—æ®µ
    if 'params' not in sampler_config:
        sampler_config['params'] = {}
    
    # ç¡®ä¿ params ä¸­æœ‰ guider_config
    if 'guider_config' not in sampler_config['params']:
        sampler_config['params']['guider_config'] = {}
    
    # è®¾ç½® guider
    if getattr(args, "guider", None):
        guider_target = f"dit.sampling.guiders.{args.guider}"
        sampler_config['params']['guider_config']['target'] = guider_target
        print(f"[DiffusionEngine] âœ… Setting guider in sampler config: {guider_target}")
        print(f"[DiffusionEngine] Updated sampler config: {sampler_config}")
        
    if getattr(args, "guiderscale", None) is not None:
        sampler_config['params']['guider_config']['params'] = sampler_config['params']['guider_config'].get('params', {})
        sampler_config['params']['guider_config']['params']['scale'] = args.guiderscale
        print(f"[DiffusionEngine] âœ… Setting guider scale in sampler config: {args.guiderscale}")
        print(f"[DiffusionEngine] Updated sampler config: {sampler_config}")
    
    # å®ä¾‹åŒ– sampler
    self.sampler = instantiate_from_config(sampler_config)
    print(f"[DiffusionEngine] Sampler initialized: {self.sampler}")
```

## 6. å‘½ä»¤è¡Œå‚æ•°é…ç½®

```python
# æ–‡ä»¶ä½ç½®: generate_t2i_sr.py
def add_sample_specific_args(parser):
    # ... [å…¶ä»–å‚æ•°çœç•¥]
    
    # âœ… æ·»åŠ  guider å‚æ•°
    group.add_argument('--guider', type=str, default=None,
                       help='Override guider class (e.g. VanillaCFG, IdentityGuider, TaylorSeerGuider)')
    group.add_argument('--guiderscale', type=float, default=None, help='Override CFG/TaylorSeer scale')
    
    return parser
```

## 7. è·³æ­¥åˆ¤æ–­æµç¨‹

1. **å…¥å£ç‚¹**ï¼šå½“ `BaseDiffusionSampler.denoise()` è¢«è°ƒç”¨æ—¶ï¼Œé¦–å…ˆè°ƒç”¨ `guider.prepare_inputs`
   
2. **åˆ¤æ–­æ˜¯å¦è·³æ­¥**ï¼š
   ```python
   if self.use_cache and self.has_cache_for(sigma):
       self._skip_cur_step = True
       return None, None, None, None
   ```
   
3. **è·³æ­¥æ‰§è¡Œ**ï¼š
   ```python
   if getattr(self.guider, "_skip_cur_step", False):
       denoised = self.guider(None, sigma)
   ```
   
4. **ç¼“å­˜å‘½ä¸­**ï¼šåœ¨ `__call__` æ–¹æ³•ä¸­ï¼Œå¦‚æœä¼ å…¥ `x=None`ï¼Œä¼šä»ç¼“å­˜ä¸­è·å–ç»“æœï¼š
   ```python
   if self.use_cache and step_key in self.cache:
       return self.cache[step_key]
   ```

5. **ç¼“å­˜ç”Ÿæˆ**ï¼šæ¯æ¬¡æ‰§è¡ŒçœŸå®è®¡ç®—åï¼Œå°†ç»“æœä¿å­˜åˆ°ç¼“å­˜å¹¶å¯èƒ½é€šè¿‡å¤–æ¨ç”Ÿæˆé¢å¤–ç¼“å­˜ï¼š
   ```python
   if len(self.history) >= 2:
       # ... è®¡ç®—å¤–æ¨é¢„æµ‹
       extrapolated = e1 + (e1 - e0)
       self.cache[s2] = extrapolated.detach()
   ```

## 8. å®Œæ•´è°ƒç”¨æµç¨‹

1. é€šè¿‡å‘½ä»¤è¡Œå‚æ•° `--guider TaylorSeerGuider --guiderscale å€¼` å¯ç”¨ TaylorSeer

2. DiffusionEngine åˆå§‹åŒ–æ—¶ï¼Œå°†è¿™äº›å‚æ•°ä¼ é€’ç»™ sampler é…ç½®

3. é‡‡æ ·æ—¶çš„æ‰§è¡Œé¡ºåºï¼š
   - `DiffusionEngine.sample()`
   - â†’ åˆ›å»º `wrapped_denoiser` å‡½æ•°
   - â†’ è°ƒç”¨ `self.sampler(denoiser=wrapped_denoiser, ...)`
   - â†’ é‡‡æ ·å™¨å†…éƒ¨å¾ªç¯è°ƒç”¨ `denoise(x, denoiser, sigma, ...)`
   - â†’ è°ƒç”¨ `guider.prepare_inputs` åˆ¤æ–­æ˜¯å¦å¯ä»¥è·³æ­¥
   - â†’ å¦‚æœå¯ä»¥è·³æ­¥ï¼Œç›´æ¥è¿”å›ç¼“å­˜ç»“æœï¼›å¦åˆ™æ‰§è¡ŒçœŸå®è®¡ç®—

4. è·³æ­¥æ¡ä»¶ï¼š
   - å½“å‰ sigma å€¼åœ¨ç¼“å­˜ä¸­å­˜åœ¨
   - ä½¿ç”¨å¯ç”¨äº†ç¼“å­˜ (use_cache=True)

5. ç¼“å­˜ç”Ÿæˆæ–¹å¼ï¼š
   - çœŸå®è®¡ç®—åä¿å­˜åˆ°ç¼“å­˜
   - å½“è¿ç»­ä¸¤ä¸ªæ—¶é—´æ­¥çš„ sigma å·®å€¼å°äºé˜ˆå€¼æ—¶ï¼Œå¤–æ¨è®¡ç®—ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥
